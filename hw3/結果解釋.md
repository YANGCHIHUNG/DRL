MAB 演算法模擬結果解釋

以下圖示為在 1,000 輪模擬中，四種策略（ε-Greedy、UCB1、Softmax、Thompson Sampling）的累積回報曲線：

![模擬結果](static/output.png)

1. ε-Greedy (ε=0.1)

- 最終累積回報約 1,600，領先所有方法。

- 以 90% 機率利用當前最優臂、10% 探索，其快速鎖定高均值臂的特性在本高斯回報場景中特別有效。

2. Softmax (τ=0.1)

- 最終累積回報約 1,000。

- 根據估計值做 Boltzmann 取樣，τ=0.1 時類似較強貪婪但保留隨機性，前期探索足夠，後期可集中在優質臂。

3. UCB1

- 最終累積回報約 900。

- 初期依賴置信界鼓勵探索，隨時間對數衰減後集中利用，因此成長曲線相對平緩。

4. Thompson Sampling

- - 最終累積回報約 800。

示範中使用二元回報模型 (0/1)，與其他連續高斯回報算法不同尺度，學習速度較慢。若改為 Gaussian–Gaussian TS，表現通常可追近或超越 UCB1。

分析與建議

1. 參數調校：

- ε-Greedy 的 ε、Softmax 的 τ 等可透過多組模擬、平均結果選出最優值。
2. 
回報模型一致性：

- 為公平比較，建議讓所有算法使用相同回報分布；例如改用連續高斯的 Thompson Sampling。

3. 重複實驗：

- 建議重複多次模擬並取平均，減少單次隨機性影響。

4. 長期行為觀察：

- 固定 ε 的 ε-Greedy 後期仍持續探索，長期後悔可能線性增長，可考慮採 ε 衰減策略。

- UCB1、Gaussian TS 等在理論上具有對數級後悔保證，適合極端長期任務。

以上解釋可作為未來在不同場景下選擇或改進 MAB 演算法的依據。